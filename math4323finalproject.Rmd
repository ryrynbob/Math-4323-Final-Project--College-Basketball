---
title: "4323 Final Project- College Basketball Dataset"
author: "Ryan Nguyen and Justin George"
date: "2023-04-25"
output:
  ioslides_presentation: default
  slidy_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Background on our Dataset

This is a dataset that contains information about college basketball teams from 2013 to 2019, including the team's performance statistics as well as their postseason performance. The dataset contains 3455 observations with 24 variables, including 5 categorical var4iables and 19 numerical variables. 

My project as a whole aims to use supervised learning to predict the stage at which a given college basketball team will be eliminated from the NCAA March Madness Tournament. The response variable is the categorical variable POSTSEASON, which represents the stage which the team was eliminated or where their season ended. We plan to use K-nearest neighbor classifier, support vector machines, and other categorical supervised learning algorithms to develop the classification model.



## Ryan: Conducting Principal Component Analysis (PCA)

Since we are using a large dataset with all the stats on college basketball teams during one season, and the high dimensionality of our data, we want to make sure that our PCA does its best to determine which variables account for the most variance to find the ones that are most significant. This will help predict our responses most accurately.

Here are some key points to note: \
- PCA will come in use for other two models we are running \
- PCA will be tested with and without effect \
- Some models with complete dataset; some models with limited amount

## Slide with R Output

```{r cbb dataset}
library(readr)
#our data set
data <- read.csv("C:/Users/justi/Downloads/archive/cbb.csv")
data
#select only the numerical variables
num_vars <- c("G", "W", "ADJOE", "ADJDE", "BARTHAG", "EFG_O", "EFG_D", "TOR", "TORD", "ORB", "DRB", "FTR", "FTRD", "2P_O", "2P_D", "3P_O", "3P_D", "ADJ_T", "WAB")
#Standardize the data
data_std <- scale(data[, 3:21])
```

## Slide with Continued R Output

```{r results}
#Perform PCA
pca <- prcomp(data_std, center = TRUE, scale = FALSE)
#Print Summary of PCA
summary(pca)
#Plot cumulative proportion of variance explained by each principal component
plot(cumsum(pca$sdev^2 / sum(pca$sdev^2)), xlab = "Number of principal components", ylab = "Cumulative proportion of variance explained")
#Choose number of principal components to keep
num_components <- 5
#Compute the new dataset with the selected number of principal components
data_pca <- predict(pca, newdata = data_std)[, 1:num_components]
data_pca
```

## Justin: Conducting KNN (K-Nearest-Neighbors)

Since we do have data on the predictor that we are using to base our analysis on, we can call this research as supervised learning. Based on that, one key task we want to accomplish is creating classification models that can help with the prediction of the final stage a college basketball team makes. KNN, or K-nearest-neighbors, is an excellent way to perform this. 

## 