---
title: "4323 Final Project- College Basketball Dataset"
author: "Ryan Nguyen and Justin George"
date: "2023-04-25"
output:
  slidy_presentation: default
  ioslides_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Background on our Dataset

This is a dataset that contains information about college basketball teams from 2013 to 2019, including the team's performance statistics as well as their postseason performance. The dataset contains 3455 observations with 24 variables, including 5 categorical var4iables and 19 numerical variables. 

My project as a whole aims to use supervised learning to predict the stage at which a given college basketball team will be eliminated from the NCAA March Madness Tournament. The response variable is the categorical variable POSTSEASON, which represents the stage which the team was eliminated or where their season ended. We plan to use K-nearest neighbor classifier, support vector machines, and other categorical supervised learning algorithms to develop the classification model.



## Ryan: Conducting Principal Component Analysis (PCA)

Since we are using a large dataset with all the stats on college basketball teams during one season, and the high dimensionality of our data, we want to make sure that our PCA does its best to determine which variables account for the most variance to find the ones that are most significant. This will help predict our responses most accurately.

Here are some key points to note: \
- PCA will come in use for other two models we are running \
- PCA will be tested with and without effect \
- Some models with complete dataset; some models with limited amount

## Slide with R Output

```{r cbb dataset}
library(readr)
#our data set
data <- read.csv("C:/Users/Justin/Downloads/archive (1)/cbb.csv")
data
#select only the numerical variables
num_vars <- c("G", "W", "ADJOE", "ADJDE", "BARTHAG", "EFG_O", "EFG_D", "TOR", "TORD", "ORB", "DRB", "FTR", "FTRD", "X2P_O", "X2P_D", "X3P_O", "X3P_D", "ADJ_T", "WAB")
#Standardize the data
data_std = data
data_std[,3:21] <- scale(data[, 3:21])
```

## Slide with Continued R Output

```{r results}
#Perform PCA
pca <- prcomp(data_std[,3:21], center = TRUE, scale = FALSE)
#Print Summary of PCA
summary(pca)
#Plot cumulative proportion of variance explained by each principal component
plot(cumsum(pca$sdev^2 / sum(pca$sdev^2)), xlab = "Number of principal components", ylab = "Cumulative proportion of variance explained")
#Choose number of principal components to keep
num_components <- 5
#Compute the new dataset with the selected number of principal components
data_pca <- predict(pca, newdata = data_std)[, 1:num_components]
data_pca
```

## Justin: Conducting KNN (K-Nearest-Neighbors)

Since we do have data on the predictor that we are using to base our analysis on, we can call this research as supervised learning. Based on that, one key task we want to accomplish is creating classification models that can help with the prediction of the final stage a college basketball team makes. KNN, or K-nearest-neighbors, is an excellent way to perform this. 

## Formatting predictor variable

```{r knn code, echo=TRUE}
library(class)
library(ISLR)
data$SEED[is.na(data$SEED)] = 17

#Encode POSTSEASON as follows:

data$POSTSEASON <- ifelse(data$POSTSEASON == "Champions", 1,
                          ifelse(data$POSTSEASON == "2ND", 2,
                                 ifelse(data$POSTSEASON == "F4", 3,
                                        ifelse(data$POSTSEASON == "E8", 4,
                                               ifelse(data$POSTSEASON == "S16", 5,
                                                      ifelse(data$POSTSEASON == "R32", 6,
                                                             ifelse(data$POSTSEASON == "R64", 7,
                                                                    ifelse(data$POSTSEASON == "R68", 8,NA))))))))
data$POSTSEASON[is.na(data$POSTSEASON)] = 9
```

## Creating train and test sets
 
```{r, echo = TRUE}
n <- nrow(data)
RNGkind(sample.kind = "Rounding")
set.seed(1)
train <- sample(1:n,1455)

X.train <- data[train, -c(1,2,22)]
X.test <- data[-train, -c(1,2,22)]
y.train<- data$POSTSEASON[train]
y.test <- data$POSTSEASON[-train]
```

## KNN: K = 1 to 10

```{r}
knn.test.err = numeric(length(10))
for(K in c(1:10)){
  set.seed(1)
  print(paste("K: ", K, sep = ""))
  knn.pred = knn(X.train,X.test,y.train,k = K)
  print(paste("Test error: ", mean(knn.pred != y.test, sep = "")))
  print(table(knn.pred,y.test))
  knn.test.err[K] = mean(knn.pred != y.test)
}
```

## Results

```{r, echo = TRUE}
min(knn.test.err)
which.min(knn.test.err)
```

## Explanation

Through KNN validation set, we can see that the lowest test error among K = 1-10 is 0.145, with K = 6. This is a very good number to have result as it shows over 85% of the data is being properly classified by the KNN model. 


## LOOCV train and test sets

```{r loocv, echo=TRUE}
X.train <- data[, -c(1,2,22)]
y.train <- data[, "POSTSEASON"]
```

## LOOCV: K = 1 to 10

```{r}
knn.test.err = numeric(length(10))
for(K in c(1:10)){
  set.seed(1)
  print(paste("K: ", K, sep = ""))
  knn.pred = knn.cv(train = X.train, cl = y.train, k = K)
  print(paste("Test error: ", mean(knn.pred != y.train, sep = "")))
  print(table(knn.pred,y.train))
  knn.test.err[K] = mean(knn.pred != y.train)
}
```

## Results

```{r, echo = TRUE}
min(knn.test.err)
which.min(knn.test.err)
```

## Explanation

Through LOOCV, we can see that the lowest test error among K = 1-10 is 0.1389, with K = 10. This is a very good number to have result as it shows over 86% of the data is being properly classified by the KNN model.

## K-fold Cross-Validation setup

```{r, echo = TRUE}
library(caret)
data$POSTSEASON = as.character(data$POSTSEASON)
set.seed(1)
train.control = trainControl(method = "CV", 5)
```

## K-fold Cross-Validation

```{r, echo = TRUE}
set.seed(1)
train(POSTSEASON~G + W + ADJOE + ADJDE + BARTHAG + EFG_O + EFG_D + TOR + TORD + ORB + DRB + FTR + FTRD + X2P_O + X2P_D + X3P_O + X3P_D + ADJ_T + WAB, data = data, method = "knn", tuneGrid = expand.grid(k = 1:10), trControl = train.control, metric = "Accuracy")
```

## Explanation

Through K-fold Cross-Validation, we see that the highest accuracy among K = 1-10 folds is 0.8318, testing on the 10th fold.

## Conclusion

So far in our project we have attempted a PCA analysis as well as training a K-Nearest-Neighbors model through R. The PCA showed that roughly the first five Principal Components are the main ones to pay attention to as they explain the majority of the variance among all the PCs. The KNN model showed that it is relatively good at classifying basketball teams with their proper final bracket round. Through cross validation methods, we generally see the model performing with similar metrics, although Leave-One-Out Cross Validation has a slightly lower test error than the other methods. Our project will continue with other models being trained such as SVM. \ Note: this KNN model was performed without PCA taken into account. Our project will look at both models in comparison.